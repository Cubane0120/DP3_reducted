bash: /SSDa/dongwoo_nam/hsh/DP3_reducted/local_conda/dp3_reducted/lib/libtinfo.so.6: no version information available (required by bash)
Data already exists at ../../3D-Diffusion-Policy/data/metaworld_push-back_expert.zarr
If you want to overwrite, delete the existing directory first.
Do you want to overwrite? (y/n)
Overwriting ../../3D-Diffusion-Policy/data/metaworld_push-back_expert.zarr
[MetaWorldEnv] use_point_crop: True
target: [0.01431474 0.68900305 0.01993605]
raw tail: [ 0.          0.          0.         -0.09206625  0.6801537   0.01993605]
raw.shape: (39,) mw_obs.shape: (39,) raw==mw_obs: False
Number of episodes : 10
POLICY: SawyerPushBackV2Policy
action min/max: 0.0 0.2062776148674158
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.023061636186194445, 'in_place_reward': 0.16242039446911546, 'obj_to_target': 0.20572535893154295, 'unscaled_reward': 0.020610516464737626}
Episode: 0 failed with reward 1.9539054562202942 and success times 0.0
action min/max: -0.057304574115632924 0.24546553766224144
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01657602864408505, 'in_place_reward': 0.16574095319092427, 'obj_to_target': 0.1982943864301473, 'unscaled_reward': 0.01529950473461253}
Episode: 0 failed with reward 1.8034117477644223 and success times 0.0
action min/max: -0.10070876446842793 0.21200209670930337
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02409560770300353, 'in_place_reward': 0.18991092095208104, 'obj_to_target': 0.1604860538672087, 'unscaled_reward': 0.02184982053666119}
Episode: 0 failed with reward 2.1990400855738317 and success times 0.0
action min/max: 0.0 0.20470564750307008
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.022827545133971092, 'in_place_reward': 0.17347054011857443, 'obj_to_target': 0.18355626012052662, 'unscaled_reward': 0.02058825040032522}
Episode: 0 failed with reward 1.8608714728978313 and success times 0.0
action min/max: -0.0295925128084939 0.23700911371753453
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.017281963794930216, 'in_place_reward': 0.16596882603883847, 'obj_to_target': 0.19781141660969476, 'unscaled_reward': 0.015901026338303023}
Episode: 0 failed with reward 1.7387833126803012 and success times 0.0
action min/max: -0.05365383544744138 0.21355691451440728
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02219528068884109, 'in_place_reward': 0.17234323949239191, 'obj_to_target': 0.18551316377656343, 'unscaled_reward': 0.020057365178305836}
Episode: 0 failed with reward 1.9312537045230138 and success times 0.0
action min/max: -0.012176889031425251 0.2025511112603281
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02330586559082387, 'in_place_reward': 0.19425068822782446, 'obj_to_target': 0.15570838217641328, 'unscaled_reward': 0.02125143757766955}
Episode: 0 failed with reward 1.894602330040451 and success times 0.0
action min/max: 0.0 0.21517715176250551
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.021781030950997305, 'in_place_reward': 0.19211640156879653, 'obj_to_target': 0.15800334032514673, 'unscaled_reward': 0.01995343398508353}
Episode: 0 failed with reward 1.982482599770789 and success times 0.0
action min/max: 0.0 0.24878401461977506
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.014799169990207077, 'in_place_reward': 0.16525831264504162, 'obj_to_target': 0.19932827466627337, 'unscaled_reward': 0.013769839084726397}
Episode: 0 failed with reward 1.6746944192712088 and success times 0.0
action min/max: -0.031405222360246586 0.21531637566827833
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.021242448606068616, 'in_place_reward': 0.1754565621367663, 'obj_to_target': 0.18024810659248933, 'unscaled_reward': 0.019314352596712367}
Episode: 0 failed with reward 1.846534743907522 and success times 0.0
action min/max: 0.0 0.20346757049980335
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02287104051694227, 'in_place_reward': 0.1896863680327684, 'obj_to_target': 0.16074549420565779, 'unscaled_reward': 0.020835384186110386}
Episode: 0 failed with reward 1.876288910083109 and success times 0.0
action min/max: 0.0 0.23828785739267677
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01651603021694759, 'in_place_reward': 0.1661571551682257, 'obj_to_target': 0.19741472654942901, 'unscaled_reward': 0.015251892033040764}
Episode: 0 failed with reward 1.7008211079434779 and success times 0.0
action min/max: -0.03448875412412073 0.2101838477226673
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.0221498379417618, 'in_place_reward': 0.19446131265039163, 'obj_to_target': 0.1554873649506103, 'unscaled_reward': 0.020288309780925187}
Episode: 0 failed with reward 1.8932387461064117 and success times 0.0
action min/max: -0.06981979118434799 0.2078364071296921
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02385224664557192, 'in_place_reward': 0.1691439655710291, 'obj_to_target': 0.1914073590998712, 'unscaled_reward': 0.02135068930423926}
Episode: 0 failed with reward 2.0369639160830277 and success times 0.0
action min/max: 0.0 0.2186924692202291
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.021849899815790698, 'in_place_reward': 0.16963499658686387, 'obj_to_target': 0.19046819619770589, 'unscaled_reward': 0.01973873407851392}
Episode: 0 failed with reward 2.037999671393857 and success times 0.0
action min/max: -0.025099614827553646 0.24822630125523026
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.014938417938718368, 'in_place_reward': 0.15704773971087288, 'obj_to_target': 0.21955006383262968, 'unscaled_reward': 0.013829540268148487}
Episode: 0 failed with reward 1.6822487186903425 and success times 0.0
action min/max: -0.09808224937499854 0.23993962207286224
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.019290812779126983, 'in_place_reward': 0.18424559924668302, 'obj_to_target': 0.16744277924976167, 'unscaled_reward': 0.0177728217233434}
Episode: 0 failed with reward 2.045276482215062 and success times 0.0
action min/max: -0.10043986005652687 0.213137791666546
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02406870987727701, 'in_place_reward': 0.15378796218280483, 'obj_to_target': 0.22926391720294848, 'unscaled_reward': 0.021253904375722513}
Episode: 0 failed with reward 2.1562068966662826 and success times 0.0
action min/max: 0.0 0.21867206652105742
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02058032722809863, 'in_place_reward': 0.18326761004214281, 'obj_to_target': 0.16873702608803307, 'unscaled_reward': 0.018851351742992302}
Episode: 0 failed with reward 1.8514409670363035 and success times 0.0
action min/max: -0.09302880181663642 0.24228879337253628
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.018827556976830275, 'in_place_reward': 0.15437612386103633, 'obj_to_target': 0.22742627659900308, 'unscaled_reward': 0.017067374132746272}
Episode: 0 failed with reward 1.9823956424836344 and success times 0.0
action min/max: -0.10312930547785966 0.22350034255403195
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.022511959943387016, 'in_place_reward': 0.1517516905761575, 'obj_to_target': 0.23594449757468142, 'unscaled_reward': 0.019995782955360832}
Episode: 0 failed with reward 2.1167101438106615 and success times 0.0
action min/max: -0.0065752817551091485 0.23378623877293447
DONE at step 200 info: /SSDa/dongwoo_nam/hsh/DP3_reducted/third_party/gym-0.21.0/gym/logger.py:34: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  warnings.warn(colorize("%s: %s" % ("WARN", msg % args), "yellow"))
{'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01735950436226741, 'in_place_reward': 0.17466799235944414, 'obj_to_target': 0.18154101024754374, 'unscaled_reward': 0.016043515484919346}
Episode: 0 failed with reward 1.717291256048766 and success times 0.0
action min/max: -0.05549923062244714 0.2071018051852086
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.023473628484393057, 'in_place_reward': 0.1930608052655594, 'obj_to_target': 0.1569752246358582, 'unscaled_reward': 0.021376330763772607}
Episode: 0 failed with reward 1.9925125219815525 and success times 0.0
action min/max: -0.09900803987228307 0.23795384225484617
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01970131677395922, 'in_place_reward': 0.15664950379357873, 'obj_to_target': 0.22067761847888034, 'unscaled_reward': 0.017812069845947145}
Episode: 0 failed with reward 2.033420220954511 and success times 0.0
action min/max: 0.0 0.22711966990431876
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.019104724849589874, 'in_place_reward': 0.19597384168149745, 'obj_to_target': 0.15392788079861894, 'unscaled_reward': 0.017716111712530968}
Episode: 0 failed with reward 1.8178798442894555 and success times 0.0
action min/max: 0.0 0.23393444974474953
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.017610454749619444, 'in_place_reward': 0.16282779437973768, 'obj_to_target': 0.20477207764190836, 'unscaled_reward': 0.016148329940690893}
Episode: 0 failed with reward 1.7583558814712674 and success times 0.0
action min/max: -0.06473657727907446 0.22385945999302315
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02082851359073621, 'in_place_reward': 0.17589499846580312, 'obj_to_target': 0.17954064593016247, 'unscaled_reward': 0.018976658190117162}
Episode: 0 failed with reward 1.9328249918443834 and success times 0.0
action min/max: -0.07325306789485254 0.2306164391019615
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.02010874233485304, 'in_place_reward': 0.1644365687755552, 'obj_to_target': 0.20112359726799245, 'unscaled_reward': 0.018244517417994727}
Episode: 0 failed with reward 1.9363608784446915 and success times 0.0
action min/max: -0.04468688708431925 0.208053436585505
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.023115168143863652, 'in_place_reward': 0.16894673723523276, 'obj_to_target': 0.1917882804614041, 'unscaled_reward': 0.020755215179811746}
Episode: 0 failed with reward 1.9286735087061666 and success times 0.0
action min/max: -0.059156022273877416 0.22555609170124025
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.020395709533812964, 'in_place_reward': 0.15876901158594278, 'obj_to_target': 0.21484985724393788, 'unscaled_reward': 0.018406586709875557}
Episode: 0 failed with reward 1.8867367666910757 and success times 0.0
action min/max: -0.06309899057148491 0.22636784756610429
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.020340959337812107, 'in_place_reward': 0.15149804133773276, 'obj_to_target': 0.23681321526055077, 'unscaled_reward': 0.018260626018695658}
Episode: 0 failed with reward 1.892929297770639 and success times 0.0
action min/max: 0.0 0.2097210637854705
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.0221978552439397, 'in_place_reward': 0.16179237534155788, 'obj_to_target': 0.20721910080517053, 'unscaled_reward': 0.01990835853142519}
Episode: 0 failed with reward 1.888054353150677 and success times 0.0
action min/max: -0.007910602854834765 0.22119505829649022
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.019812669335572332, 'in_place_reward': 0.17780730553631272, 'obj_to_target': 0.1765462179218111, 'unscaled_reward': 0.018149867612774866}
Episode: 0 failed with reward 1.7774195915818687 and success times 0.0
action min/max: -0.05942101885135667 0.23205147234633616
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.01901273700772632, 'in_place_reward': 0.17814916070708808, 'obj_to_target': 0.176026025004909, 'unscaled_reward': 0.01747958576405582}
Episode: 0 failed with reward 1.870967301076626 and success times 0.0
action min/max: 0.0 0.22839060853002413
DONE at step 200 info: {'success': 0.0, 'near_object': 0.0, 'grasp_success': 0.0, 'grasp_reward': 0.020259477361396508, 'in_place_reward': 0.1412204762017023, 'obj_to_target': 0.2808956875054182, 'unscaled_reward': 0.01803727696478224}
Episode: 0 failed with reward 1.973862369969938 and success times 0.0
action min/max: -0.012711177772499877 0.22591339656349696
Found 8 GPUs for rendering. Using device 0.
Traceback (most recent call last):
  File "gen_demonstration_expert.py", line 212, in <module>
    main(args)
  File "gen_demonstration_expert.py", line 117, in main
    obs_dict, reward, done, info = e.step(action)
  File "/SSDa/dongwoo_nam/hsh/DP3_reducted/3D-Diffusion-Policy/diffusion_policy_3d/env/metaworld/metaworld_wrapper.py", line 187, in step
    point_cloud, depth = self.get_point_cloud()
  File "/SSDa/dongwoo_nam/hsh/DP3_reducted/3D-Diffusion-Policy/diffusion_policy_3d/env/metaworld/metaworld_wrapper.py", line 155, in get_point_cloud
    point_cloud = point_cloud_sampling(point_cloud, self.num_points, 'fps')
  File "/SSDa/dongwoo_nam/hsh/DP3_reducted/3D-Diffusion-Policy/diffusion_policy_3d/gym_util/mjpc_wrapper.py", line 79, in point_cloud_sampling
    _, sampled_indices = torch3d_ops.sample_farthest_points(points=point_cloud[...,:3], K=num_points)
  File "/SSDa/dongwoo_nam/hsh/DP3_reducted/third_party/pytorch3d_simplified/pytorch3d/ops/sample_farthest_points.py", line 139, in sample_farthest_points
    idx = _C.sample_farthest_points(points, lengths, K, start_idxs)
KeyboardInterrupt
